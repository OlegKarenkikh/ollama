# vim: filetype=dockerfile
# Ollama Minimal V2 - Финальный образ, использует промежуточный слой
# Тег: minimal-v2

FROM olegkarenkikh/ollama:minimal-v2-intermediate

LABEL maintainer="DevOps" \
      version="1.0.0-minimal-v2-final" \
      description="Ollama LLM minimal runtime CUDA 12 AMD64 (final)" \
      platform="linux/amd64" \
      cuda.version="12.8"

# Непривилегированный пользователь
RUN groupadd -r -g 1000 ollama && \
    useradd -r -u 1000 -g ollama -s /bin/false -c "Ollama Service User" ollama && \
    mkdir -p /home/ollama/.ollama/models && \
    chown -R ollama:ollama /home/ollama /usr/lib/ollama && \
    echo "OLLAMA_HOST=0.0.0.0:11434" > /home/ollama/.ollama/config && \
    chown ollama:ollama /home/ollama/.ollama/config

WORKDIR /home/ollama
USER ollama

ENV PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    OLLAMA_HOST=0.0.0.0:11434 \
    OLLAMA_MODELS=/home/ollama/.ollama/models \
    HOME=/home/ollama

HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD curl -f http://localhost:11434/api/tags || exit 1

EXPOSE 11434

ENTRYPOINT ["/usr/bin/ollama"]
CMD ["serve"]
